{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7575e8d-d5cd-4dde-910a-9c803b92441a",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84baf1a5-1a5a-40be-8d05-c8eccdd1918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd728bb0-ee94-45fe-b2a8-c094a9630a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a29b2d-b3a7-4c62-ac2a-3ebbfd627e72",
   "metadata": {},
   "source": [
    "#### Defining the training environment and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9209171c-73e5-4c9d-8952-75794b0f25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Device for training\n",
    "split = 'train'\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000\n",
    "eval_interval = 200\n",
    "eval_iters =100\n",
    "\n",
    "# architecture parameters\n",
    "max_vocab_size = 256 #  only consider the most frequent \"max_vocab_size\" tokens\n",
    "vocab_size = max_vocab_size # ctual number of tokens in the vocabulary\n",
    "block_size = 16 # length of the input sequence the model is designed to handle\n",
    "n_embd = 32 # Embedding size\n",
    "num_heads = 2 # multi-headed attention\n",
    "n_layers = 2 # Number of Blocks\n",
    "ff_scale_factor = 4\n",
    "dropout = 0.0\n",
    "\n",
    "# calculating the head size\n",
    "head_size = n_embd // num_heads\n",
    "# assesing that the number heads times size of head is equal to the embedding size\n",
    "assert (num_heads * head_size) == n_embd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bb2f9-16a4-442e-88f9-3b774d84e559",
   "metadata": {},
   "source": [
    "## A function to visualize the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09ba73-c8bf-4a29-9b99-0cd94969b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdeings, name, vocab):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # plotting the data points\n",
    "    ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "    # label\n",
    "    for j, label in enumerate(name):\n",
    "        i = vocab.get_stoi()[label]\n",
    "        ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "    # axis label\n",
    "    ax.set_xlabel('X label')\n",
    "    ax.s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
